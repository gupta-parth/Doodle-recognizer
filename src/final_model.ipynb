{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final doodle recognition model \n",
    "\n",
    "Model will be trained on 100 classes from Google's QuickDraw dataset. The classes from chosen as a random sample from the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['drums', 'sun', 'laptop', 'dog', 'baseball_bat', 'ladder', 'eyeglasses', 'grapes', 'book', 'dumbbell', 'traffic_light', 'wristwatch', 'wheel', 'shovel', 'bread', 'table', 'tennis_racquet', 'cloud', 'chair', 'headphones', 'face', 'eye', 'airplane', 'snake', 'lollipop', 'guitar', 'pants', 'mushroom', 'star', 'sword', 'clock', 'hot_dog', 'syringe', 'stop_sign', 'mountain', 'smiley_face', 'apple', 'bed', 'shorts', 'broom', 'diving_board', 'flower', 'spider', 'cell_phone', 'car', 'camera', 'tree', 'square', 'moon', 'radio', 'hat', 'pizza', 'axe', 'door', 'tent', 'umbrella', 'line', 'cup', 'fan', 'triangle', 'basketball', 'pillow', 'scissors', 't-shirt', 'tooth', 'alarm_clock', 'paper_clip', 'spoon', 'microphone', 'candle', 'pencil', 'envelope', 'saw', 'frying_pan', 'screwdriver', 'helmet', 'bridge', 'light_bulb', 'ceiling_fan', 'key', 'donut', 'bird', 'circle', 'beard', 'coffee_cup', 'butterfly', 'bench', 'rifle', 'cat', 'sock', 'ice_cream', 'moustache', 'suitcase', 'hammer', 'rainbow', 'knife', 'cookie', 'baseball', 'lightning', 'bicycle']\n"
     ]
    }
   ],
   "source": [
    "f = open(\"../classes.txt\")\n",
    "categories = f.readlines()\n",
    "f.close()\n",
    "categories = [category.replace('\\n', '').replace(' ', '') for category in categories]\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/drums.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/sun.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/laptop.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/dog.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/baseball%20bat.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/ladder.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/eyeglasses.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/grapes.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/book.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/dumbbell.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/traffic%20light.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/wristwatch.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/wheel.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/shovel.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/bread.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/table.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/tennis%20racquet.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/cloud.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/chair.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/headphones.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/face.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/eye.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/airplane.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/snake.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/lollipop.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/guitar.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/pants.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/mushroom.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/star.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/sword.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/clock.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/hot%20dog.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/syringe.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/stop%20sign.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/mountain.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/smiley%20face.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/apple.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/bed.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/shorts.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/broom.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/diving%20board.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/flower.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/spider.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/cell%20phone.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/car.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/camera.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/tree.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/square.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/moon.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/radio.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/hat.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/pizza.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/axe.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/door.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/tent.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/umbrella.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/line.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/cup.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/fan.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/triangle.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/basketball.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/pillow.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/scissors.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/t-shirt.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/tooth.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/alarm%20clock.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/paper%20clip.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/spoon.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/microphone.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/candle.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/pencil.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/envelope.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/saw.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/frying%20pan.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/screwdriver.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/helmet.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/bridge.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/light%20bulb.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/ceiling%20fan.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/key.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/donut.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/bird.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/circle.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/beard.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/coffee%20cup.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/butterfly.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/bench.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/rifle.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/cat.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/sock.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/ice%20cream.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/moustache.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/suitcase.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/hammer.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/rainbow.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/knife.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/cookie.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/baseball.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/lightning.npy\n",
      "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/bicycle.npy\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "!mkdir data\n",
    "def download_data():\n",
    "    url = 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/'\n",
    "    for category in categories:\n",
    "        label_name = category.replace('_', '%20')\n",
    "        path = url + label_name + '.npy'\n",
    "        print(path)\n",
    "        urllib.request.urlretrieve(path, 'data/'+category+'.npy')\n",
    "\n",
    "download_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "\n",
    "Using 4000 images per class as training takes a long time on this local computer (around 7 mins per epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path, split_ratio=0.2, class_size=4000):\n",
    "    data_folder = glob.glob(os.path.join(path, '*.npy'))\n",
    "    x = np.empty([0, 784])   # Because each image is 28 * 28\n",
    "    y = np.empty([0])\n",
    "    class_names = []\n",
    "    for i, file_name in enumerate(data_folder):\n",
    "        data = np.load(file_name)\n",
    "        data = data[0:class_size, :]\n",
    "        labels = np.full(data.shape[0], i)\n",
    "\n",
    "        x = np.concatenate((x, data), axis=0)\n",
    "        y = np.append(y, labels)\n",
    "\n",
    "        class_name, ext = os.path.splitext(os.path.basename(file_name))\n",
    "        class_names.append(class_name)\n",
    "    data = None\n",
    "    labels = None\n",
    "\n",
    "    # Data randomization\n",
    "    perm = np.random.permutation(y.shape[0])\n",
    "    x = x[perm, :]\n",
    "    y = y[perm]\n",
    "\n",
    "    # Split set\n",
    "    split_size = int(x.shape[0]/100*(split_ratio*100))\n",
    "\n",
    "    x_test = x[0:split_size, :]\n",
    "    y_test = y[0:split_size]\n",
    "\n",
    "    x_train = x[split_size:x.shape[0], :]\n",
    "    y_train = y[split_size:y.shape[0]]\n",
    "\n",
    "    return x_train, y_train, x_test, y_test, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320000\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test, class_names = load_data('data')\n",
    "print(len(x_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "line\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "plt.imshow(x_train[4000].reshape(28, 28))\n",
    "print(class_names[int(y_train[4000].item())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 28\n",
    "x_train = x_train.reshape(x_train.shape[0], IMAGE_SIZE, IMAGE_SIZE, 1).astype('float32')\n",
    "x_test = x_test.reshape(x_test.shape[0], IMAGE_SIZE, IMAGE_SIZE, 1).astype('float32')\n",
    "\n",
    "# Normalize\n",
    "x_train /= 255.0\n",
    "x_test /= 255.0\n",
    "\n",
    "# One hot encoding\n",
    "y_train = keras.utils.to_categorical(y_train, len(class_names))\n",
    "y_test = keras.utils.to_categorical(y_test, len(class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN():\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Convolution2D(16, (3, 3),\n",
    "                            padding='same',\n",
    "                            input_shape=x_train.shape[1:], activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(layers.Convolution2D(32, (3, 3), padding='same', activation= 'relu'))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(layers.Convolution2D(64, (3, 3), padding='same', activation= 'relu'))\n",
    "    model.add(layers.MaxPooling2D(pool_size =(2,2)))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dense(100, activation='softmax')) \n",
    "    # Train model\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                optimizer='adam',\n",
    "                metrics=['top_k_categorical_accuracy'])  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 28, 28, 16)        160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 14, 14, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 7, 7, 64)          18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 3, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               73856     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100)               12900     \n",
      "=================================================================\n",
      "Total params: 110,052\n",
      "Trainable params: 110,052\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = CNN()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 288000 samples, validate on 32000 samples\n",
      "Epoch 1/5\n",
      "288000/288000 - 460s - loss: 1.9857 - top_k_categorical_accuracy: 0.7706 - val_loss: 1.4209 - val_top_k_categorical_accuracy: 0.8703\n",
      "Epoch 2/5\n",
      "288000/288000 - 463s - loss: 1.2808 - top_k_categorical_accuracy: 0.8877 - val_loss: 1.2005 - val_top_k_categorical_accuracy: 0.8957\n",
      "Epoch 3/5\n",
      "288000/288000 - 470s - loss: 1.1093 - top_k_categorical_accuracy: 0.9071 - val_loss: 1.0828 - val_top_k_categorical_accuracy: 0.9106\n",
      "Epoch 4/5\n",
      "288000/288000 - 465s - loss: 1.0161 - top_k_categorical_accuracy: 0.9173 - val_loss: 1.0214 - val_top_k_categorical_accuracy: 0.9172\n",
      "Epoch 5/5\n",
      "288000/288000 - 468s - loss: 0.9536 - top_k_categorical_accuracy: 0.9235 - val_loss: 0.9884 - val_top_k_categorical_accuracy: 0.9197\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, validation_split=0.1, batch_size=256, verbose=2, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:91.86750054359436\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"Accuracy:{score[1]*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a48f42d6b2cc564c84f407a5f0014b33de5d29db06b959bd1beac822a38c3b71"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
